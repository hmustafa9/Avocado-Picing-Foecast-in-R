---
title: "Avocado Pricing in R"
author: "Husnain Mustafa, Rudraksh Tyagi"
date: "06/12/2024"
format:
  html:
    code-fold: true
    highlight-style: github
theme: darkly

---

# Loading Data:

## Loading Libraries and Data Files
```{r}
#| message = FALSE
library(tidyverse) 
library(ggplot2)
library(naniar)
```
## Reading and Displaying the Avocado Data
In this analysis, we start by loading the readr package to read a CSV file named avocado.csv. The code snippet reads the data into a dataframe and displays the first few rows of the dataset to give an initial overview of its contents.

```{r}
#| message = FALSE
data <- read_csv('data/avocado.csv')
head(data)
```

# Dataset Infomation:
## Inspecting the Dimensions of the Avocado Data
After reading the avocado dataset into a dataframe, the next step involves inspecting the dimensions of the dataset. This step helps us understand the size of the data, i.e., the number of rows and columns it contains.
```{r}
dim(data)
```
## Inspecting the Column Names of the Avocado Data
Following the examination of the dataset's dimensions, the next step is to inspect the column names. This step helps us understand the different variables and features present in the dataset.
```{r}
colnames(data)
```
## Inspecting the Structure of the Avocado Data
The next step is to inspect the structure of the dataset. Look for data ranges and types.
```{r}
str(data)
```
## Missing Data:

In this step, we load additional libraries and create a heatmap to visualize any missing data within the avocado dataset. This visualization helps identify patterns of missing values and understand the extent of missing data in the dataset
```{r}
#| message = FALSE
# Load necessary libraries
library(ggplot2)
library(reshape2)

# Assuming 'data' is your dataframe
# Melt the dataframe to long format
data_melt <- melt(is.na(data))

# Create a heatmap
ggplot(data_melt, aes(x = Var2, y = Var1, fill = value)) + 
  geom_tile() + 
  scale_fill_manual(values = c("TRUE" = "purple", "FALSE" = "black")) + 
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1), 
        legend.position = "none")
```

## Generating a Numerical Summary of the Avocado Data.

After visualizing the missing data, the next step involves generating a numerical summary of the dataset. This summary provides statistical insights into the distribution and central tendencies of the numerical variables in the avocado dataset.
```{r}
data %>% 
  summary()
```
In this step, we load additional libraries and manipulate the avocado dataset to aggregate the data by week. This involves converting date columns, arranging data, creating a week column, and summarizing numeric columns.

```{r}
#| message = FALSE
library(lubridate)

# Create a deep copy of the data frame
df1 <- data

# Convert the 'Date' column to Date type
df1$Date <- ymd(df1$Date)

# Arrange the data by Date and create a week column
df1 <- df1 %>% 
  arrange(Date) %>% 
  mutate(week = floor_date(Date, "week"))

# Group by the week column and summarise the numeric columns
df1 <- df1 %>%
  group_by(week) %>% 
  summarise(across(where(is.numeric), ~sum(., na.rm = TRUE)))

# Display the first few rows of the resulting data frame
head(df1)
```
# Plotting Data distribution for Average Price:

We create visualizations for understanding the distribution and trends of avocado prices:
1.⁠ ⁠Distribution of Average Prices:
- Original Data: A histogram and density plot show the frequency and shape of average prices in the original dataset, indicating common price ranges and their distribution.
- Weekly Aggregated Data: Similar visualization for the weekly aggregated data allows comparison with the original dataset, highlighting any changes in distribution due to aggregation.
2.⁠ ⁠Trend of Average Prices Over Time:
- Line Plot: Visualizes the trend of average prices over time using weekly aggregated data, revealing patterns, seasonality, and long-term trends in avocado prices.
These visualizations help identify key patterns in price distribution and trends, providing valuable insights into the avocado market
```{r}
#| message = FALSE
# Load necessary libraries
library(ggplot2)
library(gridExtra)

# Assuming 'data' and 'df1' are your dataframes

# Color palette
color1 <- c('#296C92', '#3EB489')

# First subplot: Distribution of AveragePrice in 'data'
p1 <- ggplot(data, aes(x = AveragePrice)) +
  geom_histogram(aes(y = ..density..), bins = 30, fill = color1[1], alpha = 0.6) +
  geom_density(color = color1[1], size = 1) +
  ggtitle('Distribution: AveragePrice') +
  theme_minimal()

# Second subplot: Distribution of AveragePrice in 'df1'
p2 <- ggplot(df1, aes(x = AveragePrice)) +
  geom_histogram(aes(y = ..density..), bins = 30, fill = color1[2], alpha = 0.6) +
  geom_density(color = color1[2], size = 1) +
  ggtitle('Distribution: AveragePrice') +
  theme_minimal()

# Arrange the first two plots side by side
grid.arrange(p1, p2, ncol = 2)

# Third subplot: Line plot of AveragePrice vs Date in 'df1'
# Assuming 'Date' is a column in 'df1'
p3 <- ggplot(df1, aes(x = week, y = AveragePrice, group = 1)) +
  geom_line(color = color1[1]) +
  ggtitle('AveragePrice vs Date') +
  theme_minimal()

# Display the third plot
grid.arrange(p3, ncol = 1)


```
## Listing columns by data types:

In this step, we categorize the features of the avocado dataset into categorical and numerical features based on the uniqueness of their values. This classification is crucial for applying appropriate data analysis and visualization techniques to each type of feature. (Listing columns by data types)

```{r}
# Get column names
col <- names(data)

# Initialize lists to store categorical and numerical features
categorical_features <- c()
numerical_features <- c()

# Iterate through columns
for (i in col) {
    if (length(unique(data[[i]])) > 6) {
        numerical_features <- c(numerical_features, i)
    } else {
        categorical_features <- c(categorical_features, i)
    }
}

# Remove specific features from numerical features
numerical_features <- numerical_features[!numerical_features %in% c('Date', 'AveragePrice', 'region')]

# Print categorical and numerical features
cat('Categorical Features:', paste(categorical_features, collapse = ', '), '\n')
cat('Numerical Features:', paste(numerical_features, collapse = ', '), '\n')
```

## Visualizing Categorical Feature Distributions and Counts:

In this step, we create visualizations that display the distributions and counts of categorical features in the avocado dataset to gain insights into their patterns and frequencies:
- Density Plots: Show the distribution of each categorical feature, highlighting how often different categories occur.
- Bar Plots: Display the count of each category, providing a clear view of the frequency of each categorical value.

```{r}
#| message = FALSE
# Load necessary libraries
library(ggplot2)
library(gridExtra)

# Convert 'type' column to factor using LabelEncoder
data$type <- as.factor(data$type)

# Create subplots for distribution and count plots
p_distribution <- lapply(categorical_features, function(feature) {
  ggplot(data, aes(x = !!sym(feature))) +
    geom_density(fill = "#296C92", alpha = 0.6) +
    ggtitle(paste("Distribution:", feature)) +
    theme_minimal()
})

p_count <- lapply(categorical_features, function(feature) {
  ggplot(data, aes(x = !!sym(feature), fill = !!sym(feature))) +
    geom_bar() +
    scale_fill_manual(values = c("#296C92", "#3EB489")) +
    ggtitle(paste("Count:", feature)) +
    theme_minimal()
})

# Combine plots into grid
grid.arrange(grobs = p_distribution, ncol = 2)
grid.arrange(grobs = p_count, ncol = 2)

```

## Categorical Features vs Target Variable (AveragePrice) :

Bar plots to visualize the relationship between each categorical feature and the average price in the avocado dataset. This helps in understanding how different categories influence the average price.

- Bar Plots: Show the mean average price for each category of the categorical features, highlighting how different categories influence the average price.
- Colour-Coded: Alternating colors provide a clear visual distinction between different categories

```{r}
#| message = FALSE
# Load necessary libraries
library(ggplot2)
library(gridExtra)

# Assuming 'data' is your dataframe and 'categorical_features' is your list of categorical features
# Define your color palette
color1 <- c('#296C92', '#3EB489')

# Create a list to hold the plots
plots <- list()

# Loop through each categorical feature and create a bar plot
for (i in seq_along(categorical_features)) {
  p <- ggplot(data, aes_string(x = categorical_features[i], y = 'AveragePrice', fill = categorical_features[i])) +
    geom_bar(stat = "summary", fun = "mean", color = 'black', fill = color1[i %% length(color1) + 1]) +
    ggtitle(paste(categorical_features[i], 'vs AveragePrice')) +
    theme_minimal() +
    theme(legend.position = "none")
  
  plots[[i]] <- p
}

# Arrange the plots side by side
do.call(grid.arrange, c(plots, ncol = 2))

```
## Distribution of Numerical Features :

In this step, we create histogram and density plots to visualize the distribution of each numerical feature in the dataset. This helps us understand the distribution and variability of numerical variables
- Histogram and Density Plots: Show the distribution, central tendency, and spread of each numerical feature.
- Color-Coded: Provides clear visual distinction for each feature

```{r}
# Load necessary libraries
library(ggplot2)

# Assuming 'df1' is your dataframe and 'numerical_features' is your list of numerical features

# Determine the total number of features
total_features <- length(numerical_features)

# Determine the number of full rows needed
n_full_rows <- floor(total_features / 2)

# Loop through each numerical feature and create a distribution plot
for (i in seq(1, by = 2, length.out = n_full_rows)) {
  p1 <- ggplot(df1, aes_string(x = paste0('`', numerical_features[i], '`'))) +
    geom_histogram(aes(y = ..density..), bins = 30, fill = '#296C92', alpha = 0.6) +
    geom_density(color = '#3EB489', size = 1) +
    ggtitle(paste('Distribution:', numerical_features[i])) +
    theme_minimal()
  
  # Check if there's a second plot to add
  if (i + 1 <= total_features) {
    p2 <- ggplot(df1, aes_string(x = paste0('`', numerical_features[i + 1], '`'))) +
      geom_histogram(aes(y = ..density..), bins = 30, fill = '#296C92', alpha = 0.6) +
      geom_density(color = '#3EB489', size = 1) +
      ggtitle(paste('Distribution:', numerical_features[i + 1])) +
      theme_minimal()
    
    grid.arrange(p1, p2, ncol = 2)
  } else {
    # If there's only one plot left, print it as the last plot in a row
    print(p1)
  }
}

# If there's an odd number of features, print the last feature alone
if (total_features %% 2 != 0) {
  last_feature_index <- total_features
  last_p <- ggplot(df1, aes_string(x = paste0('`', numerical_features[last_feature_index], '`'))) +
    geom_histogram(aes(y = ..density..), bins = 30, fill = '#296C92', alpha = 0.6) +
    geom_density(color = '#3EB489', size = 1) +
    ggtitle(paste('Distribution:', numerical_features[last_feature_index])) +
    theme_minimal()
  
  print(last_p)
}



```
## Percentage contribution by feature type:
Next we calculate the percentage contributions of various numerical features based on the avocado type (conventional or organic) and visualize these contributions using a bar plot.

- Percentage Calculations: Determined the relative contributions of features like Total Volume, 4046, 4225, 4770, Total Bags, Small Bags, Large Bags, and XLarge Bags for conventional and organic avocados.
- Bar Plot Visualization: Displayed the percentage contributions side by side for easy comparison, with text labels indicating the exact percentages

```{r}
# Load necessary libraries
library(dplyr)
library(ggplot2)

# Assuming your data is in a dataframe named 'data'

# Calculate percentages for each feature based on type
total_volume <- sum(data[data$type == 'conventional',]$`Total Volume`) / sum(data$`Total Volume`) * 100
total_volume <- c(total_volume, sum(data[data$type == 'organic',]$`Total Volume`) / sum(data$`Total Volume`) * 100)

avocado_4046 <- sum(data[data$type == 'conventional',]$`4046`) / sum(data$`4046`) * 100
avocado_4046 <- c(avocado_4046, sum(data[data$type == 'organic',]$`4046`) / sum(data$`4046`) * 100)

avocado_4225 <- sum(data[data$type == 'conventional',]$`4225`) / sum(data$`4225`) * 100
avocado_4225 <- c(avocado_4225, sum(data[data$type == 'organic',]$`4225`) / sum(data$`4225`) * 100)

avocado_4770 <- sum(data[data$type == 'conventional',]$`4770`) / sum(data$`4770`) * 100
avocado_4770 <- c(avocado_4770, sum(data[data$type == 'organic',]$`4770`) / sum(data$`4770`) * 100)

total_bags <- sum(data[data$type == 'conventional',]$`Total Bags`) / sum(data$`Total Bags`) * 100
total_bags <- c(total_bags, sum(data[data$type == 'organic',]$`Total Bags`) / sum(data$`Total Bags`) * 100)

small_bags <- sum(data[data$type == 'conventional',]$`Small Bags`) / sum(data$`Small Bags`) * 100
small_bags <- c(small_bags, sum(data[data$type == 'organic',]$`Small Bags`) / sum(data$`Small Bags`) * 100)

large_bags <- sum(data[data$type == 'conventional',]$`Large Bags`) / sum(data$`Large Bags`) * 100
large_bags <- c(large_bags, sum(data[data$type == 'organic',]$`Large Bags`) / sum(data$`Large Bags`) * 100)

xlarge_bags <- sum(data[data$type == 'conventional',]$`XLarge Bags`) / sum(data$`XLarge Bags`) * 100
xlarge_bags <- c(xlarge_bags, sum(data[data$type == 'organic',]$`XLarge Bags`) / sum(data$`XLarge Bags`) * 100)

# Combine percentages into a data frame
type_numerical_features_percentage <- data.frame(
  feature = rep(c("Total Volume", "4046", "4225", "4770", "Total Bags", "Small Bags", "Large Bags", "XLarge Bags"), each = 2),
  type = rep(c("Conventional", "Organic"), times = 8),
  percentage = c(total_volume, avocado_4046, avocado_4225, avocado_4770, total_bags, small_bags, large_bags, xlarge_bags)
)

# Create the plot
ggplot(type_numerical_features_percentage, aes(x = feature, y = percentage, fill = type)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), 
            position = position_dodge(width = 0.9), 
            vjust = -0.5, size = 3) +
  labs(title = "Percentage Contribution by Feature Type", 
       fill = "Type", 
       x = "Feature", 
       y = "Percentage") +
  theme_classic() +
  theme(plot.title = element_text(hjust = 0.5))

```
## Calculating and Visualizing Yearly Percentage Contributions of Numerical Features:

Now we look at the yearly percentage contributions of various numerical features and visualize these contributions using pie charts.
- ⁠Percentage Calculations: Determined the relative contributions of features like Total Volume, 4046, 4225, 4770, Total Bags, Small Bags, Large Bags, and XLarge Bags for each year from 2015 to 2018.
- ⁠Pie Chart Visualization: Displayed the yearly percentage contributions in pie charts, with text labels indicating the exact percentages.

```{r}

# Load necessary libraries
library(dplyr)
library(ggplot2)
library(gridExtra)

# Assume `data` is already loaded as a data frame with the same structure

# Calculate percentages for each numerical feature per year
total_volume <- data %>%
  group_by(year) %>%
  summarize(Total_Volume = sum(`Total Volume`)) %>%
  mutate(Percentage = Total_Volume / sum(Total_Volume) * 100)

avocado_4046 <- data %>%
  group_by(year) %>%
  summarize(avocado_4046 = sum(`4046`)) %>%
  mutate(Percentage = avocado_4046 / sum(avocado_4046) * 100)

avocado_4225 <- data %>%
  group_by(year) %>%
  summarize(avocado_4225 = sum(`4225`)) %>%
  mutate(Percentage = avocado_4225 / sum(avocado_4225) * 100)

avocado_4770 <- data %>%
  group_by(year) %>%
  summarize(avocado_4770 = sum(`4770`)) %>%
  mutate(Percentage = avocado_4770 / sum(avocado_4770) * 100)

total_bags <- data %>%
  group_by(year) %>%
  summarize(Total_Bags = sum(`Total Bags`)) %>%
  mutate(Percentage = Total_Bags / sum(Total_Bags) * 100)

small_bags <- data %>%
  group_by(year) %>%
  summarize(Small_Bags = sum(`Small Bags`)) %>%
  mutate(Percentage = Small_Bags / sum(Small_Bags) * 100)

large_bags <- data %>%
  group_by(year) %>%
  summarize(Large_Bags = sum(`Large Bags`)) %>%
  mutate(Percentage = Large_Bags / sum(Large_Bags) * 100)

xlarge_bags <- data %>%
  group_by(year) %>%
  summarize(XLarge_Bags = sum(`XLarge Bags`)) %>%
  mutate(Percentage = XLarge_Bags / sum(XLarge_Bags) * 100)

# Combine all percentages into one data frame for easier plotting
percentage_data <- data.frame(
  year = rep(2015:2018, 8),
  feature = rep(c("Total Volume", "4046", "4225", "4770", "Total Bags", "Small Bags", "Large Bags", "XLarge Bags"), each = 4),
  percentage = c(total_volume$Percentage, avocado_4046$Percentage, avocado_4225$Percentage,
                 avocado_4770$Percentage, total_bags$Percentage, small_bags$Percentage,
                 large_bags$Percentage, xlarge_bags$Percentage)
)

# Define colors
color1 <- c("#66c2a5", "#fc8d62", "#8da0cb", "#e78ac3")

# Create labels for pie chart segments
percentage_data <- percentage_data %>%
  group_by(feature) %>%
  mutate(ypos = cumsum(percentage) - 0.5 * percentage)

# Function to create individual pie charts
create_pie_chart <- function(feature_name) {
  ggplot(percentage_data %>% filter(feature == feature_name), aes(x = "", y = percentage, fill = factor(year))) +
    geom_bar(stat = "identity", width = 1, color = "black") +
    coord_polar(theta = "y") +
    geom_text(aes(y = ypos, label = sprintf("%.1f%%", percentage)), color = "black", size = 3) +
    scale_fill_manual(values = color1) +
    theme_void() +
    theme(
      strip.text = element_text(size = 12, face = "bold"),
      legend.position = "bottom"
    ) +
    labs(fill = "Year", title = feature_name)
}

# Create individual pie charts
plots <- lapply(unique(percentage_data$feature), create_pie_chart)

# Arrange pie charts in a grid of 2
grid_plots <- lapply(seq(1, length(plots), by = 2), function(i) {
  grid.arrange(grobs = plots[i:min(i+1, length(plots))], ncol = 2)
})

# Display the plots
for (g in grid_plots) {
  print(g)
}
```

## Visualizing Time Series Data with Line Plots (time series):

Individual line plots to visualize the trends of different numerical features over time. This helps in understanding how these features change on a weekly basis
- Average Price vs Date: Shows how the average price changes over time.
- Total Volume vs Date: Visualizes the trend in total volume of avocados sold each week.
- Total Bags vs Date: Displays the weekly changes in the total number of bags sold.

```{r}
# Load necessary libraries
library(ggplot2)
library(gridExtra)

# Assume ⁠ df1 ⁠ is already loaded as a data frame with the same structure

# Create individual line plots
plot1 <- ggplot(df1, aes(x = week, y = AveragePrice)) +
  geom_line() +
  labs(title = "AveragePrice vs Date", x = "Date", y = "AveragePrice") +
  theme_minimal()

plot2 <- ggplot(df1, aes(x = week, y = `Total Volume`)) +
  geom_line() +
  labs(title = "Total Volume vs Date", x = "Date", y = "Total Volume") +
  theme_minimal()

plot3 <- ggplot(df1, aes(x = week, y = `Total Bags`)) +
  geom_line() +
  labs(title = "Total Bags vs Date", x = "Date", y = "Total Bags") +
  theme_minimal()

# Arrange plots in a single column layout
grid.arrange(plot1, plot2, plot3, ncol = 1)
```

## Visualizing Time Series Data for Multiple Features:

In this step, we create individual line plots for various numerical features to visualize their trends over time. This helps in understanding how these features change on a weekly basis.
### Features Plotted:
⁠-  ⁠4046: Sales of small-sized avocados.
-  ⁠4225: Sales of medium-sized avocados.
-⁠  ⁠4770: Sales of large-sized avocados.
-⁠  ⁠Small Bags: Sales in small bags.
-⁠  ⁠Large Bags: Sales in large bags.
-⁠  ⁠XLarge Bags: Sales in extra-large bags.
-  Line Plots: Show how these features change over time, providing a clear view of weekly trends.

```{r}
# Load necessary libraries
library(ggplot2)
library(gridExtra)

# Assume ⁠ df1 ⁠ is already loaded as a data frame with the same structure

# List of features to plot
l2 <- c('`4046`', '`4225`', '`4770`', '`Small Bags`', '`Large Bags`', '`XLarge Bags`')

# Create individual line plots
plots <- lapply(l2, function(feature) {
  ggplot(df1, aes_string(x = "week", y = feature)) +
    geom_line() +
    labs(title = paste(feature, "vs Date"), x = "Date", y = feature) +
    theme_minimal()
})

# Arrange plots in a grid layout of 3 rows and 2 columns
grid.arrange(grobs = plots, nrow = 3, ncol = 2)
```

## Visualizing Total Volume by Region and Year:

In this step, we create a point plot with lines to visualize the total volume of avocados sold across different regions and years. This helps in understanding the regional sales distribution and how it changes annually.
Total Volume vs Region by Year:
- ⁠Points and Lines: Show the total volume of avocados sold in each region for each year, highlighting annual trends and regional distribution.

```{r}
# Load necessary library
library(ggplot2)

# Define the color palette
color2 <- c('#DF3C22','#203EB9', '#F5EE04','#50CD27')

# Create the point plot with adjusted dimensions
ggplot(data, aes(y=`Total Volume`, x = region, color = factor(year), group = year)) +
  geom_point(size = 2) +
  geom_line() +
  scale_color_manual(values = color2) +
  theme_minimal() +
  labs(title = "Total Volume vs Region by Year",
       x = "Total Volume",
       y = "Region",
       color = "Year") +
  theme(legend.position = "right", legend.title = element_text(size = 12),
        plot.title = element_text(size = 16),  # Adjust title font size
        plot.background = element_rect(fill = "white"),  # Set background color
        aspect.ratio = 0.4)+  # Adjust aspect ratio to make the plot longer 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```

## Visualizing Total Volume by Region and Year:

Total Volume vs Region by Year shows the total volume of avocados sold in each region for each year, highlighting annual trends and regional distribution.


```{r}
# Load necessary library
library(ggplot2)

# Define the color palette
color2 <- c('#DF3C22','#203EB9', '#F5EE04','#50CD27')

# Create the point plot with adjusted dimensions
ggplot(data, aes(y=`Total Volume`, x = region, color = type, group = year)) +
  geom_point(size = 2) +
  geom_line() +
  scale_color_manual(values = color2) +
  theme_minimal() +
  labs(title = "Total Volume vs Region by Year",
       x = "Total Volume",
       y = "Region",
       color = "Year") +
  theme(legend.position = "right", legend.title = element_text(size = 12),
        plot.title = element_text(size = 16),  # Adjust title font size
        plot.background = element_rect(fill = "white"),  # Set background color
        aspect.ratio = 0.4)+  # Adjust aspect ratio to make the plot longer 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```

## Visualizing Total Bags by Region and Year:

Total Bags vs Region by Year to show the total number of avocado bags sold in each region for each year, highlighting annual trends and regional distribution.


```{r}
# Load necessary library
library(ggplot2)

# Define the color palette
color2 <- c('#DF3C22','#203EB9', '#F5EE04','#50CD27')

# Create the point plot with adjusted dimensions
ggplot(data, aes(y=`Total Bags`, x = region, color = factor(year), group = year)) +
  geom_point(size = 2) +
  geom_line() +
  scale_color_manual(values = color2) +
  theme_minimal() +
  labs(title = "Total Volume vs Region by Year",
       x = "Total Volume",
       y = "Region",
       color = "Year") +
  theme(legend.position = "right", legend.title = element_text(size = 12),
        plot.title = element_text(size = 16),  # Adjust title font size
        plot.background = element_rect(fill = "white"),  # Set background color
        aspect.ratio = 0.4)+  # Adjust aspect ratio to make the plot longer 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```

## Visualizing Total Bags by Region and Type:

In this step, we create a point plot with lines to visualize the total number of avocado bags sold across different regions, types (conventional or organic), and years. This helps in understanding the regional sales distribution and how it varies by avocado type annually.

```{r}
# Load necessary library
library(ggplot2)

# Define the color palette
color2 <- c('#DF3C22','#203EB9', '#F5EE04','#50CD27')

# Create the point plot with adjusted dimensions
ggplot(data, aes(y=`Total Bags`, x = region, color = type, group = year)) +
  geom_point(size = 2) +
  geom_line() +
  scale_color_manual(values = color2) +
  theme_minimal() +
  labs(title = "Total Volume vs Region by Year",
       x = "Total Volume",
       y = "Region",
       color = "Year") +
  theme(legend.position = "right", legend.title = element_text(size = 12),
        plot.title = element_text(size = 16),  # Adjust title font size
        plot.background = element_rect(fill = "white"),  # Set background color
        aspect.ratio = 0.4)+  # Adjust aspect ratio to make the plot longer 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```
## Time Series Analysis :

We began our time series analysis by inspecting the initial rows of the dataframe to ensure the data is properly structured. This confirmed that our data includes columns for date, average price, total volume, and various bag sizes.

```{r}
head(df1)
```

## Selecting and Displaying Relevant Columns:

In this step, we select specific columns from the dataframe df1 (namely AveragePrice and week) and display the first few rows of the modified dataframe to confirm the selection.

```{r}
col_names <- names(df1)
col_names <- col_names[col_names %in% c("AveragePrice","week")]

# Drop columns from df1 using indexing
df1 <- df1[, col_names]

# Show the first few rows of the modified df1
head(df1)
```

## Time Series Decomposition:

In this step, we perform a time series decomposition using a multiplicative model to analyze the trend, seasonality, and residuals in the AveragePrice data over time. This helps in understanding the underlying patterns and components of the time series data.
Output: The output will display four plots:
1.⁠ ⁠Observed: The original time series data.
2.⁠ ⁠Trend: The underlying trend component.
3.⁠ ⁠Seasonal: The seasonal component.
4.⁠ ⁠Random: The residuals or random noise component.
```{r}
library("forecast")

# Assuming df1 is your data frame with a column named 'AveragePrice'

# Convert df1['AveragePrice'] to a time series object
ts_data <- ts(df1$AveragePrice, frequency = 7)  # Assuming monthly data

# Decompose the time series using multiplicative model
decomposed <- decompose(ts_data, type = "multiplicative")

# Plot the decomposition
plot(decomposed)
```
## Testing Stationarity of the Time Series:

In this step, we test the stationarity of the AveragePrice time series using the Dickey-Fuller test and the Augmented Dickey-Fuller (ADF) test. Stationarity is a crucial property for time series data used in forecasting models like ARIMA.

```{r}
library(forecast)
library(urca)
# Define the function for testing stationarity
test_stationarity <- function(timeseries) {
  # Plot the time series
  plot(timeseries, main = "Original Time Series", ylab = "Value", xlab = "Time")
  
  # Perform Dickey-Fuller test
  df_test <- ndiffs(timeseries)
  print(paste("Number of Differences:", df_test))
  
  # Perform ADF test
  adf_test <- ur.df(timeseries, type = "trend", lags = 7, selectlags = "AIC")
  print(summary(adf_test))
}
test_stationarity(df1$AveragePrice)
```
## Log Transformation and Differencing for Time Series Decomposition:

We use log transformation and first differencing of the AveragePrice time series to stabilize the variance and achieve stationarity. We then decompose the transformed time series to analyze its components.


```{r}
# Calculate the logarithm of 'AveragePrice'
df1$Log_AveragePrice <- log(df1$AveragePrice)

# Calculate the first difference of the logarithm
df1_log_diff <- diff(df1$Log_AveragePrice)
df1_log_diff <- df1_log_diff[-1]  # Remove the NA value created by diff()

# Decompose the time series using seasonal decomposition
decomposed <- decompose(ts(df1_log_diff, frequency = 52), type = "multiplicative")

# Plot the decomposition
plot(decomposed)
```
## Testing Stationarity of the Differenced Log-Transformed Series:

We will test the stationarity of the differenced log-transformed AveragePrice time series using the previously defined test_stationarity function. Then, we will plot the original AveragePrice time series and its autocorrelation function (ACF).

```{r}
test_stationarity(df1_log_diff)
```

```{r}
ts_data <- ts(df1$AveragePrice)
plot(ts_data)
acf(ts_data, lag.max=10, main="ACF")

```

```{r}
pacf(ts_data, lag.max=10, main="PACF")
```
```{r}
loggy=ts(df1_log_diff)
plot(loggy)
acf(loggy, lag.max=10, main="ACF")
pacf(loggy, lag.max=10, main="PACF")

```
## Time Series Analysis and Forecasting with ARIMA Model:

In this analysis, we perform a series of steps to analyze and forecast the AveragePrice of avocados using an ARIMA model. The fitted values were then plotted against the original series to evaluate the model's performance. This comprehensive approach provides valuable insights into the time series behavior and helps in accurate forecasting.

```{r}
# Load necessary library
library(forecast)

# Assuming df1_log_averageprice is your time series data in R

# Fit ARIMA model
model <- Arima(df1$Log_AveragePrice, order = c(1,1,2))

# Summary of the fitted model
summary(model)

```

```{r}
# Fitted values of the model
predictions_ARIMA <- fitted(model)


predictions_ARIMA_final <- exp(predictions_ARIMA)

# Plotting the results
plot(df1$AveragePrice, type = "l", col = "blue", xlab = "Index", ylab = "Average Price", main = "Fitted Average Price")
lines(predictions_ARIMA_final, col = "red")
legend("topright", legend = c("Average Price", "Fitted Average Price"), col = c("blue", "red"), lty = 1)


```

```{r}
#| echo: false
library(forecast)

size <- length(df1$Log_AveragePrice) - 30
train <- df1$Log_AveragePrice[1:size]
test <- df1$Log_AveragePrice[(size+1):length(df1)]

cat('\t ARIMA MODEL : In - Sample Forecasting \n\n')

history <- as.vector(train)
predictions <- numeric()

for (t in 1:length(test)) {
    
    model <- Arima(history, order=c(1,1,2))
    
    output <- forecast(model, h=1)
    yhat <- output$mean
    
    predictions <- c(predictions, yhat)
    
    obs <- test[t]
    history <- c(history, obs)
    
    cat('predicted = ', exp(yhat), ', expected = ', exp(obs), '\n')
}

```
## SARIMA Model:
The SARIMA model combines both non-seasonal and seasonal components to capture the underlying patterns in the Log_AveragePrice time series. The non-seasonal component (order = c(1,1,2)) captures the short-term dependencies, while the seasonal component (seasonal = list(order = c(0,1,0), period = 52)) captures the yearly seasonal effects in the data.

By fitting this model, we can better understand and forecast the avocado prices, accounting for both trends and seasonal variations. The summary output provides insights into the model parameters and their statistical significance, helping to evaluate the model's adequacy for the given time series data.

```{r}
# Load the necessary package
library(forecast)

# Assuming df1 is your dataframe and Log_AveragePrice is the column of interest
# Fit the SARIMA model
model <- Arima(df1$Log_AveragePrice, order = c(1,1,2), seasonal = list(order = c(0,1,0), period = 52))


# Print the summary of the fitted model
summary(model)

```
```{r}
#| echo: false
# Load necessary packages
library(forecast)

# Assuming df1 is your dataframe and Log_AveragePrice is the column of interest
# Split the data into training and testing sets
size <- nrow(df1) - 30
train <- df1$Log_AveragePrice[1:size]
test <- df1$Log_AveragePrice[(size+1):nrow(df1)]

cat("\t SARIMA MODEL : In - Sample Forecasting \n")

history <- as.numeric(train)
predictions <- c()

for (t in 1:length(test)) {
  model <- Arima(history, order = c(1,1,2), seasonal = list(order = c(0,1,0), period = 52))
  output <- forecast(model, h = 1)
  yhat <- output$mean[1]
  predictions <- c(predictions, yhat)
  
  obs <- test[t]
  history <- c(history, obs)
  
  cat(sprintf("predicted = %f, expected = %f\n", exp(yhat), exp(obs)))
}

```
```{r}
# Create a time series for predictions
predictions_series <- ts(predictions, start = start(test), frequency = frequency(test))

# Plot the expected and predicted values
df1$Date <- as.Date(df1$week)  # Assuming the row names are dates
df1$ExpectedValues <- df1$AveragePrice
df1$PredictedValues <- c(rep(NA, size), exp(predictions))

ggplot(df1, aes(x = Date)) +
  geom_line(aes(y = ExpectedValues, color = "Expected Values")) +
  geom_line(aes(y = PredictedValues, color = "Predicted Values")) +
  labs(title = "SARIMA Model: Expected vs Predicted Values",
       y = "Average Price",
       x = "Date") +
  scale_color_manual(name = "Legend", values = c("Expected Values" = "blue", "Predicted Values" = "red")) +
  theme_minimal()
```


